---
title: 'Project 3: Text Analytics'
author: "Ari Boyarsky, Isaac Gritz, Abdul Sheikhnureldin, Sameer Rau, Clare Lohrmann"
date: "December 16, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part 1

*Please note that we use the `head` command in multiple places to limit the size of this file for clairty. If you would like to see the unabridged version please look at the file `PartB.R` in the zip file.*

First we get all the libraries needed.

```{r echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
# libraries
library(tm) # Framework for text mining.
library(qdap) # Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
library(dplyr) # Data wrangling, pipe operator %>%().
library(RColorBrewer) # Generate palette of colours for plots.
# library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(quanteda) # quantitative manip of corpus
library(textreuse) # text analytic feature sets


# get data
data(acq)
```

### Part A

Functions created:

`doc_search`: Enter data and a search strind and the function will return the document number, word index, and line number of the string.

We use the following functions to help comoute the results:

`inspect`: This function ispects the contents of an R Object

`findFreqTerms`: this function returns the frequency of terms in a corpus

`DocumentTermMatrix`: thsi function turns a corpus into a DocumentTermMatrix, which holds the frequency of terms based on a document frequency matrix scheme.

`wordcount`: counts the words in a corpus

`termFreq`: counts term frequency

`findAssocs`: find words that are commonly associated with a term based on a n-gram schema

`tokenize_sentecnes`: breaks a corpus up into its respective sentences

`tokenize_words`: breaks a corpus up into its respective words

`tm_map`: a powerful function from `tm` that allows us to make chnages to a corpus like chnaging it to lowercase, or removing stopwords.

`annotate`: a function that allows us to add features to words in a corpus. These features may be lexical, morphological, syntactical, etc. One example is POS tagging, based on hunt POS.


```{r echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}

# a: try functions in lecture 7
data(acq)
inspect(head(acq))
head(summary(acq), n = 10)

list.acq <- as.list(acq)
require(openNLP)

wordcount(acq[[2]])


for(i in 1:50){
   wordcount(content(acq[[i]]))
}

dtm <- DocumentTermMatrix(acq)



termFreq(acq[[1]])

dm2 <- TermDocumentMatrix(acq, control = list(wordLengths = c(1, Inf)))

assoc <- findAssocs(dm2, "states", 0.25)

freq.terms <- findFreqTerms(dm2, lowfreq = 3)
```

### Part B

```{r echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
# b: find 10 longest docs (in words)
inspect(head(acq))
dtm <- DocumentTermMatrix(acq)
wordCounts <- rowSums(as.matrix(dtm))
largest <- names(head(sort(wordCounts, decreasing = T), 10))
largest
```


### Part C

```{r echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
# c: for each doc, work through examples in lect 7 to display dendrogram and Wordcloud

# we will do this right and remove stop words
d <- Corpus(VectorSource(acq))
dm3 <- tm_map(d, removeWords, stopwords('english'))

dm2 <- TermDocumentMatrix(dm3)
dm2 <- removeSparseTerms(dm2, sparse = 0.60)
distMatrix <- dist(scale(dm2))

fit <- hclust(distMatrix, method = "ward.D2")
plot(fit)
groups <- cutree(fit, k=4)
rect.hclust(fit, k=4, border="red")

library(wordcloud)
pal <- brewer.pal(9, "BuGn")

dm3 <- TermDocumentMatrix(dm3)
m1 <- as.matrix(dm3)
word.freq <- sort(rowSums(m1), decreasing = T)
wordcloud(words = names(word.freq), freq=word.freq, min.freq = 3, random.order = F, max.words = 50, colors = pal)
```

### Part D

```{r echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
# d: find longest word and longest sentence from 10 largest docs
# longest sentence/word
sents <- tokenize_sentences(acq[[largest[1]]]$content)
word <- tokenize_words(acq[[largest[1]]]$content)
# longest word by chars
word[which.max(nchar(word))]

# longest sentence by chars
sents[which.max(nchar(sents))]

sents <- tokenize_sentences(acq[[largest[2]]]$content)
word <- tokenize_words(acq[[largest[2]]]$content)

# longest word by chars
word[which.max(nchar(word))]

# longest sentence by chars
sents[which.max(nchar(sents))]

sents <- tokenize_sentences(acq[[largest[3]]]$content)
word <- tokenize_words(acq[[largest[3]]]$content)

# longest word by chars
word[which.max(nchar(word))]

# longest sentence by chars
sents[which.max(nchar(sents))]

sents <- tokenize_sentences(acq[[largest[4]]]$content)
word <- tokenize_words(acq[[largest[1]]]$content)

# longest word by chars
word[which.max(nchar(word))]

# longest sentence by chars
sents[which.max(nchar(sents))]

sents <- tokenize_sentences(acq[[largest[1]]]$content)
word <- tokenize_words(acq[[largest[4]]]$content)

# longest word by chars
word[which.max(nchar(word))]

# longest sentence by chars
sents[which.max(nchar(sents))]

sents <- tokenize_sentences(acq[[largest[5]]]$content)
word <- tokenize_words(acq[[largest[5]]]$content)

# longest word by chars
word[which.max(nchar(word))]

# longest sentence by chars
sents[which.max(nchar(sents))]

sents <- tokenize_sentences(acq[[largest[6]]]$content)
word <- tokenize_words(acq[[largest[6]]]$content)

# longest word by chars
word[which.max(nchar(word))]

# longest sentence by chars
sents[which.max(nchar(sents))]

sents <- tokenize_sentences(acq[[largest[7]]]$content)
word <- tokenize_words(acq[[largest[7]]]$content)

# longest word by chars
word[which.max(nchar(word))]

# longest sentence by chars
sents[which.max(nchar(sents))]

sents <- tokenize_sentences(acq[[largest[8]]]$content)
word <- tokenize_words(acq[[largest[8]]]$content)

# longest word by chars
word[which.max(nchar(word))]

# longest sentence by chars
sents[which.max(nchar(sents))]

sents <- tokenize_sentences(acq[[largest[9]]]$content)
word <- tokenize_words(acq[[largest[9]]]$content)

# longest word by chars
word[which.max(nchar(word))]

# longest sentence by chars
sents[which.max(nchar(sents))]

sents <- tokenize_sentences(acq[[largest[10]]]$content)
word <- tokenize_words(acq[[largest[10]]]$content)

# longest word by chars
word[which.max(nchar(word))]

# longest sentence by chars
sents[which.max(nchar(sents))]
```

### Part E

```{r echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
# e: Print a table of the length of each sentence in each of the 10 documents.
library(formattable)
sents <- tokenize_sentences(acq[[largest[1]]]$content)
formattable(sapply(sents, wordcount))
sents <- tokenize_sentences(acq[[largest[2]]]$content)
formattable(sapply(sents, wordcount))
sents <- tokenize_sentences(acq[[largest[3]]]$content)
formattable(sapply(sents, wordcount))
sents <- tokenize_sentences(acq[[largest[4]]]$content)
formattable(sapply(sents, wordcount))
sents <- tokenize_sentences(acq[[largest[5]]]$content)
formattable(sapply(sents, wordcount))
sents <- tokenize_sentences(acq[[largest[6]]]$content)
formattable(sapply(sents, wordcount))
sents <- tokenize_sentences(acq[[largest[7]]]$content)
formattable(sapply(sents, wordcount))
sents <- tokenize_sentences(acq[[largest[8]]]$content)
formattable(sapply(sents, wordcount))
sents <- tokenize_sentences(acq[[largest[9]]]$content)
formattable(sapply(sents, wordcount))
sents <- tokenize_sentences(acq[[largest[10]]]$content)
formattable(sapply(sents, wordcount))
```

### Part F

```{r echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
# f: remove punctuation and then display sentence
no_punct <- tm_map(acq, removePunctuation)
dataframe<-data.frame(text=unlist(sapply(no_punct, `[`, "content")), stringsAsFactors=F)
as.String(dataframe$text)
```

### Part G

```{r echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
# g: pring Wordnet POS tags
library(wordnet)
library(openNLP)

tokenized <- tokenize_words(as.String(dataframe$text))

s <-as.String(dataframe$text)


posTagger <- Maxent_POS_Tag_Annotator(language = "en", probs = F, model = NULL)


sent_token_annotator <- Maxent_Sent_Token_Annotator ()
word_token_annotator <- Maxent_Word_Token_Annotator ()
pos_tag_annotator <- Maxent_POS_Tag_Annotator ()

y1 <- annotate(s, c(sent_token_annotator, word_token_annotator))

t <- annotate(s, posTagger, y1)
head(t, n = 30)
```

### Part H

```{r echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
# h: analyze word freq using zipfR
require(zipfR)
# ?zipfR

tdm <- TermDocumentMatrix(acq)
# for clarity we limit the size of this for the print our
```

```{r echo=TRUE, results="hide", message=FALSE, warning=FALSE, tidy=TRUE}
temp <- inspect(tdm)
```

```{r echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
freq <- data.frame(ST = rownames(temp), Freq = rowSums(temp))
row.names(freq) <- NULL
head(freq)
mytfl <- tfl(freq$Freq, freq$ST)
acq.spc <- tfl2spc(mytfl)

# frequency spectrum
plot(acq.spc)
summary(acq.spc)

acq.fzm <- lnre("fzm", acq.spc)
summary(acq.fzm)

# Vocab Growth
Vm(acq.fzm,1)/N(acq.spc)
sample.sizes <- floor (N(acq.spc)/100*(1:100))
acq.vgc <- vgc.interp(acq.spc, sample.sizes)
plot(acq.vgc)
```

## Part C

We create the function `doc_search` to get the document, word index, and line number of a particular string.

```{r echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
# install.packages("quanteda")
# library(quanteda)

doc_search <- function(data, search){
  mycorpus <- corpus(data)
  result <- kwic(mycorpus, search, window = 3)
  result[[1]] <- gsub("reut-000", "", result[[1]])  
  result[[1]] <- gsub(".xml", "", result[[1]]) 
  doc_number <- as.numeric(result[[1]]) # document number
  word_index <- result[[2]] # word index in document
  word <- result[[4]][[1]] # word searching for
  line_index <- list()
  docs <- unique(doc_number)
  out<-capture.output(cat(mycorpus[docs]))
  line <- grep(search, out, ignore.case = T)
  line_index <- list(line_index, as.numeric(line))
  df = data.frame(doc_number,word_index, line)
  word_used = cat("Searching documents for:", word, "\n")
  return(df)
}

doc_search(acq, "because")
doc_search(acq, "Mattress")
doc_search(acq, "appraisals")
 
```

## Part D

This porject has been essential to our education in Data Science. SPecifically, text is by far the most prolific type of data. Humans generate trillions of bytes of text everyday from speech to writing. The ability to manipulate and analyze this is key to the study of data science. Data science hopes to provide useful insight to all sorts of phenomena through the exploitation of descriptive data. If the vast majority of available data is text than understanding text analytics is vital to any education in data science. In this excercise we have had to manipulate corpora to extract basic statistical data from texts. While the excercises in this project have been generally basic they are a an important stepping stone to more powerful text analytics, such as sentiment analysis, topic modeling, language modeling, keyword extraction, and more. If we hope to learn to extract latent meaning from texts the tools we learned in this project, such as word frequency, document term matricies, word associations, term extraction, among others, are essential to this education.  
